{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "offensive-burton",
   "metadata": {},
   "source": [
    "## Datapipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "id": "soviet-demand",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(0, '..')\n",
    "\n",
    "from data_management.load_tagtog import load_from_annotations_folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "id": "applicable-english",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = load_from_annotations_folder(\"../tagtog/datasets/annotations_large_mixed_batch/\", csv_folder=\"../data_management/\")\n",
    "X, y = df['content'], df['is_flood']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "id": "southern-heating",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "643 training examples\n",
      "161 validation examples\n",
      "202 test examples\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train_valid, X_test, y_train_valid, y_test = train_test_split(X, y, test_size = 0.20, random_state = 42)\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(X_train_valid, y_train_valid, test_size = 0.20, random_state = 42)\n",
    "\n",
    "print(f\"{len(X_train)} training examples\\n{len(X_valid)} validation examples\\n{len(X_test)} test examples\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "guided-permission",
   "metadata": {},
   "source": [
    "#### Storing splits in dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "id": "regulation-disposition",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "train_dt=pd.DataFrame([X_train, y_train]).T\n",
    "valid_dt=pd.DataFrame([X_valid, y_valid]).T\n",
    "test_dt=pd.DataFrame([X_test, y_test]).T\n",
    "train_dt['type'] = 'train'\n",
    "valid_dt['type'] = 'valid'\n",
    "test_dt['type'] = 'test'\n",
    "splits_df = pd.concat([train_dt, valid_dt, test_dt])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "id": "valuable-latino",
   "metadata": {},
   "outputs": [],
   "source": [
    "splits_df.to_csv(\"training_splits.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "desperate-shift",
   "metadata": {},
   "source": [
    "#### Loading splits from google drive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "id": "invisible-hotel",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'google.colab'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-255-7b0c2cba9ae5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mgoogle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolab\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdrive\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mdrive\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'drive'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'google.colab'"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "import pandas as pd\n",
    "drive.mount('drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "norwegian-spanking",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"/content/drive/My Drive/iri_bengali_data/training_splits.csv\", index_col=0)\n",
    "train_dt = df[df['type'] == 'train']\n",
    "test_dt = df[df['type'] == 'test']\n",
    "valid_dt = df[df['type'] == 'valid']\n",
    "X_train, y_train = train_dt['content'], train_dt['is_flood']\n",
    "X_test, y_test = test_dt['content'], test_dt['is_flood']\n",
    "X_valid, y_valid = valid_dt['content'], valid_dt['is_flood']\n",
    "print(f\"{len(X_train)} training examples\\n{len(X_valid)} validation examples\\n{len(X_test)} test examples\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "contemporary-counter",
   "metadata": {},
   "source": [
    "## Testing Pipeline for sklearn models\n",
    "\n",
    "- Feature Extractor\n",
    "- Model\n",
    "- Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "steady-festival",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature Extractors\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "\n",
    "# Models\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# Metrics\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "# Combine \n",
    "from sklearn.model_selection import ParameterGrid\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "def sklearn_run_pipeline(X_train, X_valid, y_train, y_valid, vectorizer, model, metric):\n",
    "    vectorizer.fit(np.hstack([X_train, X_valid]))\n",
    "    model.fit(vectorizer.transform(X_train), y_train)\n",
    "    return metric(model.predict(vectorizer.transform(X_valid)), y_valid)\n",
    "\n",
    "def sklearn_run_pipeline_series(X_train, X_valid, y_train, y_valid, vectorizers, models, metrics):\n",
    "    res = {modelname: {} for modelname in models.keys()}\n",
    "    \n",
    "    train_with_valid = np.hstack([X_train, X_valid])\n",
    "    for vect in vectorizers.values():\n",
    "        vect.fit(train_with_valid)\n",
    "    \n",
    "    X_vects_train = {name: vect.transform(X_train) for name, vect in vectorizers.items()}\n",
    "    X_vects_valid = {name: vect.transform(X_valid) for name, vect in vectorizers.items()}\n",
    "    \n",
    "    for model_name, model in models.items():\n",
    "        table = res[model_name]\n",
    "        for vect_name, X_vect_train in X_vects_train.items():\n",
    "            model.fit(X_vect_train, y_train)\n",
    "            y_pred = model.predict(X_vects_valid[vect_name])\n",
    "            res[model_name][vect_name] = {name: metric(y_valid, y_pred) for name, metric in metrics.items()}\n",
    "    \n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "greatest-longitude",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_extractors = {\n",
    "    'CountVect': CountVectorizer(), \n",
    "    'CountVect-2gram': CountVectorizer(ngram_range = (1, 2)),\n",
    "    'CountVect-min_df-max_df': CountVectorizer(min_df = 0.05, max_df = 0.95),\n",
    "    'CountVect-2gram-min_df-max_df': CountVectorizer(min_df = 0.05, max_df = 0.95, ngram_range = (1, 2)),\n",
    "    'TFIDF': TfidfVectorizer(), \n",
    "    'TFIDF-2gram': TfidfVectorizer(ngram_range = (1, 2)),\n",
    "    'TFIDF-min_df-max_df': TfidfVectorizer(min_df = 0.05, max_df = 0.95),\n",
    "    'TFIDF-2gram-min_df-max_df': TfidfVectorizer(min_df = 0.05, max_df = 0.95, ngram_range = (1, 2))\n",
    "}\n",
    "\n",
    "models = {\n",
    "    'RandomForest': RandomForestClassifier(class_weight = 'balanced'),\n",
    "    'LinearSVC': LinearSVC(class_weight = 'balanced'),\n",
    "    'LogRegL1': LogisticRegression(penalty = 'l1', \n",
    "                                   class_weight = 'balanced', \n",
    "                                   solver = 'liblinear',\n",
    "                                   max_iter = 1000\n",
    "                                  ),\n",
    "    'LogRegL2': LogisticRegression(penalty = 'l2', \n",
    "                                   class_weight = 'balanced', \n",
    "                                   solver = 'liblinear',\n",
    "                                   max_iter = 1000\n",
    "                                  )\n",
    "}\n",
    "\n",
    "metrics = {\"Accuracy\": accuracy_score, 'Precision': precision_score, 'Recall': recall_score, 'f1': f1_score}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "preceding-builder",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = sklearn_run_pipeline_series(X_train, X_valid, y_train, y_valid, feature_extractors, models, metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "structural-adventure",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RandomForest\n",
      "|                               |   Accuracy |   Precision |   Recall |       f1 |\n",
      "|:------------------------------|-----------:|------------:|---------:|---------:|\n",
      "| CountVect                     |   0.84472  |    0.906977 | 0.65     | 0.757282 |\n",
      "| CountVect-2gram               |   0.84472  |    0.926829 | 0.633333 | 0.752475 |\n",
      "| CountVect-min_df-max_df       |   0.857143 |    0.95122  | 0.65     | 0.772277 |\n",
      "| CountVect-2gram-min_df-max_df |   0.832298 |    0.902439 | 0.616667 | 0.732673 |\n",
      "| TFIDF                         |   0.850932 |    0.973684 | 0.616667 | 0.755102 |\n",
      "| TFIDF-2gram                   |   0.832298 |    0.971429 | 0.566667 | 0.715789 |\n",
      "| TFIDF-min_df-max_df           |   0.857143 |    0.95122  | 0.65     | 0.772277 |\n",
      "| TFIDF-2gram-min_df-max_df     |   0.857143 |    0.95122  | 0.65     | 0.772277 | \n",
      "\n",
      "LinearSVC\n",
      "|                               |   Accuracy |   Precision |   Recall |       f1 |\n",
      "|:------------------------------|-----------:|------------:|---------:|---------:|\n",
      "| CountVect                     |   0.838509 |    0.826923 | 0.716667 | 0.767857 |\n",
      "| CountVect-2gram               |   0.850932 |    0.846154 | 0.733333 | 0.785714 |\n",
      "| CountVect-min_df-max_df       |   0.84472  |    0.843137 | 0.716667 | 0.774775 |\n",
      "| CountVect-2gram-min_df-max_df |   0.826087 |    0.833333 | 0.666667 | 0.740741 |\n",
      "| TFIDF                         |   0.888199 |    0.92     | 0.766667 | 0.836364 |\n",
      "| TFIDF-2gram                   |   0.875776 |    0.934783 | 0.716667 | 0.811321 |\n",
      "| TFIDF-min_df-max_df           |   0.89441  |    0.877193 | 0.833333 | 0.854701 |\n",
      "| TFIDF-2gram-min_df-max_df     |   0.888199 |    0.903846 | 0.783333 | 0.839286 | \n",
      "\n",
      "LogRegL1\n",
      "|                               |   Accuracy |   Precision |   Recall |       f1 |\n",
      "|:------------------------------|-----------:|------------:|---------:|---------:|\n",
      "| CountVect                     |   0.857143 |    0.824561 | 0.783333 | 0.803419 |\n",
      "| CountVect-2gram               |   0.826087 |    0.807692 | 0.7      | 0.75     |\n",
      "| CountVect-min_df-max_df       |   0.813665 |    0.767857 | 0.716667 | 0.741379 |\n",
      "| CountVect-2gram-min_df-max_df |   0.826087 |    0.82     | 0.683333 | 0.745455 |\n",
      "| TFIDF                         |   0.89441  |    0.877193 | 0.833333 | 0.854701 |\n",
      "| TFIDF-2gram                   |   0.888199 |    0.83871  | 0.866667 | 0.852459 |\n",
      "| TFIDF-min_df-max_df           |   0.881988 |    0.836066 | 0.85     | 0.842975 |\n",
      "| TFIDF-2gram-min_df-max_df     |   0.888199 |    0.862069 | 0.833333 | 0.847458 | \n",
      "\n",
      "LogRegL2\n",
      "|                               |   Accuracy |   Precision |   Recall |       f1 |\n",
      "|:------------------------------|-----------:|------------:|---------:|---------:|\n",
      "| CountVect                     |   0.857143 |    0.836364 | 0.766667 | 0.8      |\n",
      "| CountVect-2gram               |   0.857143 |    0.862745 | 0.733333 | 0.792793 |\n",
      "| CountVect-min_df-max_df       |   0.857143 |    0.836364 | 0.766667 | 0.8      |\n",
      "| CountVect-2gram-min_df-max_df |   0.863354 |    0.865385 | 0.75     | 0.803571 |\n",
      "| TFIDF                         |   0.869565 |    0.819672 | 0.833333 | 0.826446 |\n",
      "| TFIDF-2gram                   |   0.875776 |    0.884615 | 0.766667 | 0.821429 |\n",
      "| TFIDF-min_df-max_df           |   0.900621 |    0.854839 | 0.883333 | 0.868852 |\n",
      "| TFIDF-2gram-min_df-max_df     |   0.89441  |    0.852459 | 0.866667 | 0.859504 | \n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "for model, model_results in results.items():\n",
    "    print(model)\n",
    "    print(pd.DataFrame(model_results).T.to_markdown(), '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "asian-isaac",
   "metadata": {},
   "source": [
    "## BERT Model with Translation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "parallel-belgium",
   "metadata": {},
   "source": [
    "### Translate Dataset, Kinda Hard :( Translate broke"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "industrial-patrol",
   "metadata": {},
   "outputs": [],
   "source": [
    "from basicBanglaTools import translate"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "disciplinary-retirement",
   "metadata": {},
   "source": [
    "## Bert Based Models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "reported-wallace",
   "metadata": {},
   "source": [
    "### Multi-lingual Bert Uncased"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "hairy-layout",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-multilingual-uncased were not used when initializing BertModel: ['cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertTokenizer, BertModel\n",
    "import torch\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-multilingual-uncased')\n",
    "model = BertModel.from_pretrained(\"bert-base-multilingual-uncased\", num_labels = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "id": "wicked-associate",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader, Dataset\n",
    "import torch.nn as nn\n",
    "\n",
    "class BengaliNewsDataset(Dataset):\n",
    "    def __init__(self, X, y = None):\n",
    "        if y is None:\n",
    "            self.X, self.y = X['content'], X['is_flood']\n",
    "        else:\n",
    "            self.X, self.y = X, y\n",
    "    \n",
    "    def __getitem__(self, i):\n",
    "        return self.X.iloc[i], self.y.iloc[i]\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.y)\n",
    "\n",
    "\n",
    "def collate_batch(batch):\n",
    "    text, labels = zip(*batch)\n",
    "    text_enc = tokenizer(text, padding=\"max_length\", truncation=True, return_tensors = \"pt\")\n",
    "    labels_enc = torch.tensor([1 if label else 0 for label in labels])\n",
    "    \n",
    "    return text_enc, labels_enc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "id": "bound-supervisor",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BERT_Based_Classifier(nn.Module):\n",
    "    def __init__(self, bert_model):\n",
    "        super(BERT_Based_Classifier, self).__init__()\n",
    "        \n",
    "        self.bert = bert_model\n",
    "        self.pooler_size = bert_model.pooler.dense.out_features\n",
    "        \n",
    "        self.classifier = nn.Sequential(nn.Dropout(p = 0.1, inplace = False),\n",
    "                                        nn.Linear(in_features=self.pooler_size, out_features=2, bias=True)\n",
    "                                       )\n",
    "        \n",
    "    def forward(self, **params):\n",
    "        output = self.bert(**params)['pooler_output']\n",
    "        logits = self.classifier(output)\n",
    "        return logits\n",
    "    \n",
    "    def train(self):\n",
    "        self.bert.train()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "id": "unique-wyoming",
   "metadata": {},
   "outputs": [],
   "source": [
    "classification_model = BERT_Based_Classifier(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "thirty-cornwall",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_sample = X.sample(10)\n",
    "y_sample = y[X_sample.index]\n",
    "\n",
    "\n",
    "ds = BengaliNewsDataset(X_sample, y_sample)\n",
    "dataloader = DataLoader(ds, batch_size = 2, shuffle=True, collate_fn = collate_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "happy-pacific",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 512])"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next(iter(dataloader))[0]['input_ids'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "id": "vocational-shopper",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import get_scheduler, AdamW\n",
    "\n",
    "optimizer = AdamW(model.parameters(), lr=5e-5)\n",
    "\n",
    "num_epochs = 2\n",
    "num_training_steps = num_epochs * len(dataloader)\n",
    "lr_scheduler = get_scheduler(\n",
    "    \"linear\",\n",
    "    optimizer=optimizer,\n",
    "    num_warmup_steps=0,\n",
    "    num_training_steps=num_training_steps\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "id": "authentic-destruction",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "61e46fd788e141c896cd1677a438ce47",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/15 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from tqdm.auto import tqdm\n",
    "\n",
    "progress_bar = tqdm(range(num_training_steps))\n",
    "\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "classification_model.train()\n",
    "for epoch in range(num_epochs):\n",
    "    for X, y in dataloader:\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        logits_pred = classification_model(**X)\n",
    "        loss = criterion(logits_pred, y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        lr_scheduler.step()\n",
    "        progress_bar.update(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "scientific-parent",
   "metadata": {},
   "outputs": [],
   "source": [
    "        outputs = self.bert(**params)\n",
    "        \n",
    "        hidden_states = outputs[1]\n",
    "        \n",
    "        pooled_output = torch.cat(tuple(hidden_states[-self.use_hidden:]), dim=-1)\n",
    "        print(pooled_output.shape)\n",
    "        return pooled_output\n",
    "        \n",
    "#        pooled_output = pooled_output[:, 0, :]\n",
    "        #pooled_output = self.dropout(pooled_output)\n",
    "        # classifier of course has to be 4 * hidden_dim, because we concat 4 layers\n",
    "#        logits = self.classifier(pooled_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "id": "graduate-africa",
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs = model(**batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "id": "certified-final",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 768])"
      ]
     },
     "execution_count": 185,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs['pooler_output']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "id": "comparative-adobe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.1368, -0.0041],\n",
       "        [ 0.1194,  0.0491]], grad_fn=<AddmmBackward>)"
      ]
     },
     "execution_count": 190,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classification_model.classifier(outputs['pooler_output'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "id": "tribal-proposition",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([10, 3]), torch.Size([3]))"
      ]
     },
     "execution_count": 202,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#loss = nn.BCEWithLogitsLoss()\n",
    "inp = torch.randn((10, 3), requires_grad=True)\n",
    "targ = torch.empty(3).random_(2)\n",
    "inp.shape, targ.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "economic-indicator",
   "metadata": {},
   "source": [
    "### Bangla Bert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "healthy-taxation",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertForMaskedLM, BertTokenizer, pipeline\n",
    "\n",
    "model = BertForMaskedLM.from_pretrained(\"sagorsarker/bangla-bert-base\")\n",
    "tokenizer = BertTokenizer.from_pretrained(\"sagorsarker/bangla-bert-base\")\n",
    "nlp = pipeline('fill-mask', model=model, tokenizer=tokenizer)\n",
    "for pred in nlp(f\"আমি বাংলায় {nlp.tokenizer.mask_token} গাই।\"):\n",
    "    print(pred)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
