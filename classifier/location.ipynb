{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import os\n",
    "import dateparser\n",
    "import pandas as pd\n",
    "import json\n",
    "import nltk\n",
    "import numpy as np\n",
    "import string\n",
    "import re\n",
    "import csv\n",
    "from datetime import datetime\n",
    "from collections import defaultdict\n",
    "import matplotlib.pyplot as plt\n",
    "plt.rcParams.update({'font.size': 12})\n",
    "from functions import *\n",
    "import dateparser\n",
    "from fuzzywuzzy import process\n",
    "from ccfMitchell import ccf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import stanza"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_data = load_data_tagtog(['../tagtog/output']).fillna('')\n",
    "df_data = df_data.loc[df_data['is_flood'].astype(bool)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>doc_id</th>\n",
       "      <th>filename</th>\n",
       "      <th>text</th>\n",
       "      <th>is_flood</th>\n",
       "      <th>is_bangladesh</th>\n",
       "      <th>flood_related</th>\n",
       "      <th>flood_climatechange</th>\n",
       "      <th>newspaper</th>\n",
       "      <th>flood_type</th>\n",
       "      <th>dates</th>\n",
       "      <th>...</th>\n",
       "      <th>event_damage-people_affected</th>\n",
       "      <th>event_damage-peopled_displaced</th>\n",
       "      <th>event_damage-homes_affected</th>\n",
       "      <th>event_damage-disease</th>\n",
       "      <th>event_damage-fatalities</th>\n",
       "      <th>event_dates</th>\n",
       "      <th>event_dates-date</th>\n",
       "      <th>event_dates-prev_date</th>\n",
       "      <th>datePublished</th>\n",
       "      <th>connect_filename</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ec583817-3c60-41ee-b856-65f0d9bd7772</td>\n",
       "      <td>dailySun_data_ec583817-3c60-41ee-b856-65f0d9bd...</td>\n",
       "      <td>Date Published:2017-08-31 06:03:11+00:00 tuesd...</td>\n",
       "      <td>True</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>dailySun</td>\n",
       "      <td></td>\n",
       "      <td>[]</td>\n",
       "      <td>...</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>2017-08-31 06:03:11+00:00</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>f4806621-b874-4f20-97fb-f7c1fa94f6bc</td>\n",
       "      <td>theDailyStar_data_f4806621-b874-4f20-97fb-f7c1...</td>\n",
       "      <td>Date Published:2016-05-05 00:00:00 Flash flood...</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>theDailyStar</td>\n",
       "      <td>flash</td>\n",
       "      <td>[]</td>\n",
       "      <td>...</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>2016-05-05 00:00:00</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>259d503d-f6b1-44b6-a866-8eff03799a07</td>\n",
       "      <td>prothomalo_data_259d503d-f6b1-44b6-a866-8eff03...</td>\n",
       "      <td>Date Published:None Flood situation worsened f...</td>\n",
       "      <td>True</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>prothomalo</td>\n",
       "      <td></td>\n",
       "      <td>[]</td>\n",
       "      <td>...</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>2019-07-17T03:35:13Z</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1e8c824c-6142-496e-ab4b-aeefaeedc681</td>\n",
       "      <td>theIndependent_data_1e8c824c-6142-496e-ab4b-ae...</td>\n",
       "      <td>Date Published:2016-06-04 10:27:51 Rail commun...</td>\n",
       "      <td>True</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>theIndependent</td>\n",
       "      <td></td>\n",
       "      <td>[]</td>\n",
       "      <td>...</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>2016-06-04 10:27:51</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7c2bc791-c162-46bf-9224-2686d6798c33</td>\n",
       "      <td>dailySun_data_7c2bc791-c162-46bf-9224-2686d679...</td>\n",
       "      <td>Date Published:2019-09-30 13:44:33+00:00 tuesd...</td>\n",
       "      <td>True</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>dailySun</td>\n",
       "      <td></td>\n",
       "      <td>[]</td>\n",
       "      <td>...</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>2019-09-30 13:44:33+00:00</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 28 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                 doc_id  \\\n",
       "0  ec583817-3c60-41ee-b856-65f0d9bd7772   \n",
       "1  f4806621-b874-4f20-97fb-f7c1fa94f6bc   \n",
       "2  259d503d-f6b1-44b6-a866-8eff03799a07   \n",
       "3  1e8c824c-6142-496e-ab4b-aeefaeedc681   \n",
       "4  7c2bc791-c162-46bf-9224-2686d6798c33   \n",
       "\n",
       "                                            filename  \\\n",
       "0  dailySun_data_ec583817-3c60-41ee-b856-65f0d9bd...   \n",
       "1  theDailyStar_data_f4806621-b874-4f20-97fb-f7c1...   \n",
       "2  prothomalo_data_259d503d-f6b1-44b6-a866-8eff03...   \n",
       "3  theIndependent_data_1e8c824c-6142-496e-ab4b-ae...   \n",
       "4  dailySun_data_7c2bc791-c162-46bf-9224-2686d679...   \n",
       "\n",
       "                                                text  is_flood is_bangladesh  \\\n",
       "0  Date Published:2017-08-31 06:03:11+00:00 tuesd...      True                 \n",
       "1  Date Published:2016-05-05 00:00:00 Flash flood...      True          True   \n",
       "2  Date Published:None Flood situation worsened f...      True                 \n",
       "3  Date Published:2016-06-04 10:27:51 Rail commun...      True                 \n",
       "4  Date Published:2019-09-30 13:44:33+00:00 tuesd...      True                 \n",
       "\n",
       "  flood_related flood_climatechange       newspaper flood_type dates  ...  \\\n",
       "0                                          dailySun               []  ...   \n",
       "1          True               False    theDailyStar      flash    []  ...   \n",
       "2                                        prothomalo               []  ...   \n",
       "3                                    theIndependent               []  ...   \n",
       "4                                          dailySun               []  ...   \n",
       "\n",
       "  event_damage-people_affected event_damage-peopled_displaced  \\\n",
       "0                           []                             []   \n",
       "1                           []                             []   \n",
       "2                           []                             []   \n",
       "3                           []                             []   \n",
       "4                           []                             []   \n",
       "\n",
       "  event_damage-homes_affected event_damage-disease event_damage-fatalities  \\\n",
       "0                          []                   []                      []   \n",
       "1                          []                   []                      []   \n",
       "2                          []                   []                      []   \n",
       "3                          []                   []                      []   \n",
       "4                          []                   []                      []   \n",
       "\n",
       "  event_dates event_dates-date event_dates-prev_date  \\\n",
       "0          []               []                    []   \n",
       "1          []               []                    []   \n",
       "2          []               []                    []   \n",
       "3          []               []                    []   \n",
       "4          []               []                    []   \n",
       "\n",
       "               datePublished connect_filename  \n",
       "0  2017-08-31 06:03:11+00:00                   \n",
       "1        2016-05-05 00:00:00                   \n",
       "2       2019-07-17T03:35:13Z                   \n",
       "3        2016-06-04 10:27:51                   \n",
       "4  2019-09-30 13:44:33+00:00                   \n",
       "\n",
       "[5 rows x 28 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 2723 - old data\n",
    "# 3464 - new data\n",
    "# 3074 - new new data\n",
    "override=False\n",
    "if not override and os.path.exists('timeseries_data/all_data/all_isFlood.json'):\n",
    "    df_data = pd.read_json('timeseries_data/all_data/all_isFlood.json')\n",
    "else:\n",
    "    df_data = add_prev_true_data(df_data)\n",
    "    df_data = add_newspapers(df_data)\n",
    "    df_data = add_datePublished(df_data)\n",
    "    df_data = add_location(df_data)\n",
    "    df_data = df_data[df_data['divisions'].apply(lambda x:len(x)>0)]\n",
    "    json.dump(json.loads(df_data.to_json(orient='records')), open('timeseries_data/all_data/all_isFlood.json','w'), indent=2)\n",
    "df_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('No Date Present', 'None') None\n",
      "('No Date Present', 'None') None\n",
      "('No Date Present', 'None') None\n",
      "('No Date Present', 'None') None\n",
      "('No Date Present', 'None') None\n",
      "('No Date Present', 'None') None\n",
      "('No Date Present', 'None') None\n",
      "('No Date Present', 'None') None\n",
      "('No Date Present', 'None') None\n",
      "('No Date Present', '2031-12-30 00:00:00') 2031-12-30 00:00:00\n",
      "('No Date Present', 'None') None\n",
      "('No Date Present', 'None') None\n",
      "('No Date Present', 'None') None\n",
      "('No Date Present', 'None') None\n",
      "('No Date Present', 'None') None\n",
      "('No Date Present', 'None') None\n",
      "('No Date Present', 'None') None\n",
      "('No Date Present', 'None') None\n",
      "('No Date Present', 'None') None\n",
      "('No Date Present', 'None') None\n",
      "('No Date Present', 'None') None\n",
      "('No Date Present', 'None') None\n",
      "('No Date Present', 'None') None\n",
      "('No Date Present', 'None') None\n",
      "('No Date Present', 'None') None\n",
      "('No Date Present', 'None') None\n",
      "('No Date Present', 'None') None\n",
      "('No Date Present', 'None') None\n",
      "('No Date Present', 'None') None\n",
      "('No Date Present', 'None') None\n",
      "('No Date Present', 'None') None\n",
      "('No Date Present', 'None') None\n",
      "('No Date Present', 'None') None\n",
      "('No Date Present', 'None') None\n",
      "('No Date Present', 'None') None\n",
      "('No Date Present', 'None') None\n",
      "('No Date Present', 'None') None\n",
      "('No Date Present', 'None') None\n",
      "('No Date Present', 'None') None\n",
      "('No Date Present', 'None') None\n",
      "('No Date Present', 'None') None\n",
      "('No Date Present', 'None') None\n",
      "('No Date Present', 'None') None\n",
      "('No Date Present', 'None') None\n",
      "('No Date Present', 'None') None\n",
      "('No Date Present', 'None') None\n",
      "('No Date Present', 'None') None\n",
      "('No Date Present', 'None') None\n",
      "('No Date Present', 'None') None\n",
      "('No Date Present', 'None') None\n",
      "('No Date Present', 'None') None\n",
      "('No Date Present', 'None') None\n",
      "('No Date Present', 'None') None\n",
      "('No Date Present', 'None') None\n",
      "('No Date Present', 'None') None\n",
      "('No Date Present', 'None') None\n",
      "('No Date Present', 'None') None\n",
      "('No Date Present', 'None') None\n",
      "('No Date Present', 'None') None\n",
      "('No Date Present', 'None') None\n",
      "('No Date Present', 'None') None\n",
      "('No Date Present', 'None') None\n",
      "('No Date Present', 'None') None\n",
      "('No Date Present', 'None') None\n",
      "('No Date Present', 'None') None\n",
      "('No Date Present', 'None') None\n",
      "('No Date Present', 'None') None\n",
      "('No Date Present', 'None') None\n",
      "('No Date Present', 'None') None\n",
      "('No Date Present', 'None') None\n",
      "('No Date Present', 'None') None\n",
      "('No Date Present', 'None') None\n",
      "('No Date Present', 'None') None\n",
      "('No Date Present', '2051-11-13 00:00:00') 2051-11-13 00:00:00\n",
      "('No Date Present', 'None') None\n",
      "('No Date Present', 'None') None\n",
      "('No Date Present', 'None') None\n",
      "('No Date Present', 'None') None\n",
      "('No Date Present', 'None') None\n",
      "('No Date Present', 'None') None\n",
      "('No Date Present', 'None') None\n",
      "('No Date Present', 'None') None\n",
      "('No Date Present', 'None') None\n",
      "('No Date Present', 'None') None\n",
      "('No Date Present', 'None') None\n",
      "('No Date Present', 'None') None\n",
      "('No Date Present', 'None') None\n",
      "('No Date Present', 'None') None\n",
      "('No Date Present', 'None') None\n",
      "('No Date Present', 'None') None\n",
      "('No Date Present', 'None') None\n",
      "('No Date Present', 'None') None\n",
      "('No Date Present', 'None') None\n",
      "('No Date Present', 'None') None\n",
      "('No Date Present', 'None') None\n",
      "('No Date Present', 'None') None\n",
      "('No Date Present', 'None') None\n",
      "('No Date Present', 'None') None\n",
      "('No Date Present', 'None') None\n",
      "('No Date Present', 'None') None\n",
      "('No Date Present', 'None') None\n",
      "('No Date Present', 'None') None\n",
      "('No Date Present', 'None') None\n",
      "('No Date Present', 'None') None\n",
      "('No Date Present', 'None') None\n",
      "('No Date Present', 'None') None\n",
      "('No Date Present', 'None') None\n",
      "('No Date Present', 'None') None\n",
      "('No Date Present', 'None') None\n",
      "('No Date Present', 'None') None\n",
      "('No Date Present', 'None') None\n",
      "('No Date Present', 'None') None\n",
      "('No Date Present', 'None') None\n",
      "('No Date Present', 'None') None\n",
      "('No Date Present', 'None') None\n",
      "('No Date Present', 'None') None\n",
      "('No Date Present', 'None') None\n",
      "('No Date Present', 'None') None\n",
      "('No Date Present', 'None') None\n",
      "('No Date Present', 'None') None\n",
      "('No Date Present', 'None') None\n",
      "('No Date Present', 'None') None\n",
      "('No Date Present', 'None') None\n",
      "('No Date Present', 'None') None\n",
      "('No Date Present', 'None') None\n",
      "('No Date Present', 'None') None\n",
      "('No Date Present', 'None') None\n",
      "('No Date Present', 'None') None\n",
      "('No Date Present', 'None') None\n",
      "('No Date Present', 'None') None\n",
      "('No Date Present', 'None') None\n",
      "('No Date Present', 'None') None\n",
      "('No Date Present', 'None') None\n",
      "('No Date Present', 'None') None\n",
      "('No Date Present', 'None') None\n",
      "('No Date Present', 'None') None\n",
      "('No Date Present', 'None') None\n",
      "('No Date Present', 'None') None\n",
      "('No Date Present', 'None') None\n",
      "('No Date Present', 'None') None\n",
      "('No Date Present', '2051-11-13 00:00:00') 2051-11-13 00:00:00\n",
      "('No Date Present', 'None') None\n",
      "('No Date Present', 'None') None\n",
      "('No Date Present', 'None') None\n"
     ]
    }
   ],
   "source": [
    "dates_all = query_dataframe(df_data,{'is_flood':True})['datePublished']\n",
    "dates = parse_all_dates(dates_all)\n",
    "df_data['datesP'] = dates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "datesLocation = defaultdict(set)\n",
    "for row in df_data.iterrows():\n",
    "    entry = row[1]\n",
    "    dates = entry['datesP']\n",
    "    if not dates:\n",
    "#         print(entry)\n",
    "        continue\n",
    "    year, month, day = dates['year'], dates['month'], dates['day']\n",
    "    if not year or not month or not day:\n",
    "#         print(entry)\n",
    "        continue\n",
    "    for loc in entry['districts']: datesLocation['{}-{}-{}'.format(year, month, day)].add(loc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get location entires"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import json \n",
    "import xmltodict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def extract_loc_geonames(d):\n",
    "#     return {entry['geonameId']:entry['name'] \n",
    "#              for entry in d['geonames']['geoname']}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Divisions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# r = requests.get('http://api.geonames.org/children?geonameId=1210997&username=tvp2107')\n",
    "# res = xmltodict.parse(r.text)\n",
    "# divisions = extract_loc_geonames(res)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Districts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# div_to_dist = {}\n",
    "# for geoID,name in divisions.items():\n",
    "#     r = requests.get('http://api.geonames.org/children?geonameId={}&username=tvp2107'.format(geoID))\n",
    "#     res = xmltodict.parse(r.text)\n",
    "#     div_to_dist[name] = extract_loc_geonames(res)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Upazilla"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dist_to_up = {}\n",
    "# for _,distD in div_to_dist.items():\n",
    "#     for geoID,name in distD.items():\n",
    "#         r = requests.get('http://api.geonames.org/children?geonameId={}&username=tvp2107'.format(geoID))\n",
    "#         res = xmltodict.parse(r.text)\n",
    "#         dist_to_up[name] = extract_loc_geonames(res)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "# format_path = 'timeseries_data/logistics/geeonames_{}.json'\n",
    "# json.dump(divisions, open(format_path.format('division'),'w'), indent=2)\n",
    "# json.dump(div_to_dist, open(format_path.format('div_dist'),'w'), indent=2)\n",
    "# json.dump(dist_to_up, open(format_path.format('dist_up'),'w'), indent=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "format_path = 'timeseries_data/logistics/geeonames_{}.json'\n",
    "divisions = json.load(open(format_path.format('division'),'r'))\n",
    "div_to_dist = json.load(open(format_path.format('div_dist'),'r'))\n",
    "dist_to_up = json.load(open(format_path.format('dist_up'),'r'))\n",
    "districts = [dist for _,distD in div_to_dist.items() \n",
    "                 for _,dist in distD.items()]\n",
    "upazillas = [upa for _,upaD in dist_to_up.items() \n",
    "                 for _,upa in upaD.items()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Location"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-03-15 22:36:49 INFO: Loading these models for language: en (English):\n",
      "=========================\n",
      "| Processor | Package   |\n",
      "-------------------------\n",
      "| tokenize  | combined  |\n",
      "| pos       | combined  |\n",
      "| lemma     | combined  |\n",
      "| depparse  | combined  |\n",
      "| sentiment | sstplus   |\n",
      "| ner       | ontonotes |\n",
      "=========================\n",
      "\n",
      "2021-03-15 22:36:49 INFO: Use device: cpu\n",
      "2021-03-15 22:36:49 INFO: Loading: tokenize\n",
      "2021-03-15 22:36:49 INFO: Loading: pos\n",
      "2021-03-15 22:36:50 INFO: Loading: lemma\n",
      "2021-03-15 22:36:50 INFO: Loading: depparse\n",
      "2021-03-15 22:36:50 INFO: Loading: sentiment\n",
      "2021-03-15 22:36:51 INFO: Loading: ner\n",
      "2021-03-15 22:36:52 INFO: Done loading processors!\n"
     ]
    }
   ],
   "source": [
    "nlp = stanza.Pipeline('en')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "......................................."
     ]
    }
   ],
   "source": [
    "loc_arr = {}\n",
    "filepath = 'timeseries_data/logistics/docid_locations.json'\n",
    "for i,row in enumerate(df_data.iterrows()):\n",
    "    text = row[1]['text']\n",
    "    doc_id = row[1]['doc_id']\n",
    "    doc = nlp(text)\n",
    "    arr = set()\n",
    "    try:\n",
    "        for d in doc.entities:\n",
    "            try:\n",
    "                if d.type == \"GPE\":\n",
    "                    arr.add(d.text)\n",
    "            except Exception as e:\n",
    "                print(d)\n",
    "                print(e)\n",
    "                print('------------')\n",
    "                continue\n",
    "    except Exception as e:\n",
    "        print(i)\n",
    "        print(e)\n",
    "        print('------------')\n",
    "        loc_arr[doc_id] = list(arr)\n",
    "        continue\n",
    "    loc_arr[doc_id] = list(arr)\n",
    "    if i%100==0:\n",
    "        json.dump(loc_arr, open(filepath,'w'), indent=2)\n",
    "        print('.', end='')\n",
    "json.dump(loc_arr, open(filepath,'w'), indent=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "."
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Applied processor reduces input query to empty string, all comparisons will have score 0. [Query: '']\n",
      "WARNING:root:Applied processor reduces input query to empty string, all comparisons will have score 0. [Query: '']\n",
      "WARNING:root:Applied processor reduces input query to empty string, all comparisons will have score 0. [Query: '']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "."
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Applied processor reduces input query to empty string, all comparisons will have score 0. [Query: '']\n",
      "WARNING:root:Applied processor reduces input query to empty string, all comparisons will have score 0. [Query: '']\n",
      "WARNING:root:Applied processor reduces input query to empty string, all comparisons will have score 0. [Query: '']\n",
      "WARNING:root:Applied processor reduces input query to empty string, all comparisons will have score 0. [Query: '']\n",
      "WARNING:root:Applied processor reduces input query to empty string, all comparisons will have score 0. [Query: '']\n",
      "WARNING:root:Applied processor reduces input query to empty string, all comparisons will have score 0. [Query: '']\n",
      "WARNING:root:Applied processor reduces input query to empty string, all comparisons will have score 0. [Query: '']\n",
      "WARNING:root:Applied processor reduces input query to empty string, all comparisons will have score 0. [Query: '']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "."
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Applied processor reduces input query to empty string, all comparisons will have score 0. [Query: '']\n",
      "WARNING:root:Applied processor reduces input query to empty string, all comparisons will have score 0. [Query: '']\n",
      "WARNING:root:Applied processor reduces input query to empty string, all comparisons will have score 0. [Query: '']\n",
      "WARNING:root:Applied processor reduces input query to empty string, all comparisons will have score 0. [Query: '']\n",
      "WARNING:root:Applied processor reduces input query to empty string, all comparisons will have score 0. [Query: '']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "."
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Applied processor reduces input query to empty string, all comparisons will have score 0. [Query: '']\n",
      "WARNING:root:Applied processor reduces input query to empty string, all comparisons will have score 0. [Query: '']\n",
      "WARNING:root:Applied processor reduces input query to empty string, all comparisons will have score 0. [Query: '']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "."
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Applied processor reduces input query to empty string, all comparisons will have score 0. [Query: '']\n",
      "WARNING:root:Applied processor reduces input query to empty string, all comparisons will have score 0. [Query: '']\n",
      "WARNING:root:Applied processor reduces input query to empty string, all comparisons will have score 0. [Query: '']\n",
      "WARNING:root:Applied processor reduces input query to empty string, all comparisons will have score 0. [Query: '']\n",
      "WARNING:root:Applied processor reduces input query to empty string, all comparisons will have score 0. [Query: '']\n",
      "WARNING:root:Applied processor reduces input query to empty string, all comparisons will have score 0. [Query: '']\n",
      "WARNING:root:Applied processor reduces input query to empty string, all comparisons will have score 0. [Query: '']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".................................."
     ]
    }
   ],
   "source": [
    "choices = list(divisions.values()) + districts + upazillas\n",
    "choices = [c.lower() for c in choices]\n",
    "replace_items = ['village', 'upazila', 'division', 'district']\n",
    "replace_items += [r+'s' for r in replace_items]\n",
    "loc_arr_search = []\n",
    "for i,row in enumerate(df_data.iterrows()):\n",
    "    text = row[1]['text']\n",
    "    doc_id = row[1]['doc_id']\n",
    "    if not doc_id:\n",
    "        loc_arr_search.append([])\n",
    "    else:\n",
    "        entry = loc_arr[doc_id]\n",
    "        arr = []\n",
    "        for loc in entry:\n",
    "            loc = loc.lower()\n",
    "            for r in replace_items: loc = loc.replace(r, '')\n",
    "            loc_f = process.extract(loc.lower().strip(), choices, limit=1)\n",
    "            arr.append(loc_f[0][0])\n",
    "        loc_arr_search.append(arr)\n",
    "    if i%100==0:\n",
    "        print('.', end='')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "dist_to_div, up_to_dist = {}, {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "for div,v in div_to_dist.items():\n",
    "    for _,dist in v.items():\n",
    "        dist_to_div[dist.lower()] = div.lower()\n",
    "for dist,v in dist_to_up.items():\n",
    "    for _,upa in v.items():\n",
    "        up_to_dist[upa.lower()] = dist.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "divisions_ner, districts_ner, upazilla_ner = [], [], []\n",
    "for entry in loc_arr_search:\n",
    "    div_inner, dist_inner, upa_inner = set(), set(), set()\n",
    "    for r in entry:\n",
    "        r = r.lower()\n",
    "        if r in up_to_dist:\n",
    "            upa_inner.add(r)\n",
    "            dist_inner.add(up_to_dist[r])\n",
    "            div_inner.add(dist_to_div[up_to_dist[r]])\n",
    "        elif r in dist_to_div:\n",
    "            dist_inner.add(r)\n",
    "            div_inner.add(dist_to_div[r])\n",
    "        elif r in divisions:\n",
    "            div_inner.add(r)\n",
    "    divisions_ner.append(list(div_inner))\n",
    "    districts_ner.append(list(dist_inner))\n",
    "    upazilla_ner.append(list(upa_inner))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_data['divisions_ner'] = divisions_ner\n",
    "df_data['districts_ner'] = districts_ner\n",
    "df_data['upazilla_ner'] = upazilla_ner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_data['divisions_comb'] = "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bangladesh_venv",
   "language": "python",
   "name": "bangladesh_venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
